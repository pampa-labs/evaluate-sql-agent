{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an evaluation dataset for SQL Agent\n",
    "\n",
    "This notebook is used for building an evaluation dataset containing at least:\n",
    "- `question`\n",
    "- `SQL query`\n",
    "- `natural language answer from standard agent`\n",
    "\n",
    "Evaluation dataset is based on databases obtained from **Spider dataset**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelsoria/miniconda3/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.24) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import SQLDatabase\n",
    "\n",
    "from evaluation_prompts import TARGET_PROMPT\n",
    "from utils import CustomDatabase\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the whole evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.read_json('datasets/spider/train_spider.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_databases = ['chinook_1', 'architecture']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out only interesting databases.\n",
    "\n",
    "`databases` list will contain `EvaluationDatabase` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_databases = []\n",
    "for db_name in interesting_databases:\n",
    "    evaluation_dataset = evaluation_df[evaluation_df['db_id'] == db_name]\n",
    "    database = SQLDatabase.from_uri(f'sqlite:///datasets/spider/database/{db_name}/{db_name}.sqlite')\n",
    "    evaluation_database = CustomDatabase(name=db_name, database=database, evaluation_dataset=evaluation_dataset)\n",
    "    evaluation_databases.append(evaluation_database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to query the results of the `queries` inside the `evaluation dataset` for each `evaluation database`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:00, 2941.80it/s]\n",
      "/Users/manuelsoria/Pampa/evaluate-sql-agent/utils.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.evaluation_dataset['query_result'] = results\n",
      "17it [00:00, 3902.75it/s]\n",
      "/Users/manuelsoria/Pampa/evaluate-sql-agent/utils.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.evaluation_dataset['query_result'] = results\n"
     ]
    }
   ],
   "source": [
    "for db in evaluation_databases:\n",
    "    db.run_queries()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we parse these query results into a natural language output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "target_llm_chain = LLMChain(llm=llm,prompt=TARGET_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing results for architecture...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing results for architecture: 100%|██████████| 17/17 [00:23<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used 2595 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/manuelsoria/Pampa/evaluate-sql-agent/utils.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.evaluation_dataset['nl_result'] = results\n"
     ]
    }
   ],
   "source": [
    "for db in evaluation_databases[1:]:\n",
    "    with get_openai_callback() as cb:\n",
    "        print(f\"Parsing results for {db.name}...\")\n",
    "        db.parse_query_results(target_llm_chain)\n",
    "        print(f\"Used {cb.total_tokens} tokens\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving evaluation dataset\n",
    "\n",
    "Now let's keep only relevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in evaluation_databases[1:]:\n",
    "    db.evaluation_dataset[['db_id', 'question', 'query', 'query_result', 'nl_result']].to_json(f\"datasets/custom/{db.name}_eval_dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
